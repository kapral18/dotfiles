{
  "$schema": "https://opencode.ai/config.json",
  "small_model": "openrouter/z.ai/glm-5",
  "theme": "catppuccin",
  "share": "disabled",
  "autoupdate": false,
  "permission": {
    "edit": "allow",
    "bash": "allow",
    "webfetch": "allow",
    "skill": "allow",
    "external_directory": "allow",
    "doom_loop": "ask",
  },
  "agent": {
    "plan": {
      "disable": true,
    },
    "build": {
      "disable": true,
    },
    "explore": {
      "disable": true,
    },
    "general": {
      "disable": true,
    },
    "main": {
      "description": "Main workflow agent. Can invoke sub-agents as needed to accomplish independent tasks.",
      "mode": "primary",
      "model": "openai/gpt-5.2",
      "reasoning_effort": "high",
    },
    "worker_a": {
      "description": "Implements tasks and reports concise results back to main agent",
      "mode": "subagent",
      "model": "openrouter/z.ai/glm-5",
    },
    "worker_b": {
      "description": "Second implementer for parallel work, reports concise results back to main agent",
      "mode": "subagent",
      "model": "openrouter/z.ai/glm-5",
    },
    "worker_c": {
      "description": "Third implementer for parallel work, reports concise results back to main agent",
      "mode": "subagent",
      "model": "openrouter/z.ai/glm-5",
    },
    "worker_d": {
      "description": "Fourth implementer for parallel work, reports concise results back to main agent",
      "mode": "subagent",
      "model": "openrouter/z.ai/glm-5",
    },
  },
  "default_agent": "main",
  "mcp": {
    "playwright": {
      "type": "local",
      "command": ["npx", "-y", "@playwright/mcp@latest"],
      "enabled": true,
    },
    "sequential-thinking": {
      "type": "local",
      "command": [
        "npx",
        "-y",
        "@modelcontextprotocol/server-sequential-thinking",
      ],
      "enabled": true,
    },
    // __isWork__
    "scsi-main": {
      "type": "local",
      "command": [
        "bash",
        "-lc",
        "docker run --pull=always --rm -i -e ELASTICSEARCH_CLOUD_ID=\"$(pass scsi/simian/cloud_id)\" -e ELASTICSEARCH_API_KEY=\"$(pass scsi/simian/api_key)\" -e ELASTICSEARCH_INDEX=kibana-repo simianhacker/semantic-code-search-mcp-server node dist/src/mcp_server/bin.js stdio",
      ],
      "enabled": true,
    },
    // __isWork__
    "scsi-local": {
      "type": "local",
      "command": [
        "bash",
        "-lc",
        "docker run --pull=always --rm -i -e ELASTICSEARCH_CLOUD_ID=\"$(pass scsi/k18/cloud_id)\" -e ELASTICSEARCH_API_KEY=\"$(pass scsi/k18/api_key)\" -e ELASTICSEARCH_INDEX=scsi-repo simianhacker/semantic-code-search-mcp-server node dist/src/mcp_server/bin.js stdio",
      ],
      "enabled": true,
    },
  },
  "provider": {
    "litellm": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "LiteLLM",
      "options": {
        "baseURL": "http://localhost:4000/v1",
      },
      "models": {
        "llm-gateway/gpt-5.2": {
          "name": "llm-gateway/gpt-5.2",
          "limit": {
            "output": 128000,
            "context": 400000,
          },
        },
      },
    },
  },
}
